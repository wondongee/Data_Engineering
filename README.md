# ğŸš€ Adaptive Card Data Pipeline

ERP ì‹œìŠ¤í…œ ì‚¬ìš©ìë¥¼ ìœ„í•œ Adaptive Card ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ **2ë‹¨ê³„ í ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. ë§¤ 2ì‹œê°„ë§ˆë‹¤ Sales Organization ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ ê° ì‚¬ìš©ìë³„ Adaptive Card ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì•„í‚¤í…ì²˜](#-ì•„í‚¤í…ì²˜)
- [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
- [ì„¤ì •](#-ì„¤ì •)
- [ì‚¬ìš©ë²•](#-ì‚¬ìš©ë²•)
- [ëª¨ë‹ˆí„°ë§](#-ëª¨ë‹ˆí„°ë§)
- [ê°œë°œ ê°€ì´ë“œ](#-ê°œë°œ-ê°€ì´ë“œ)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

```mermaid
graph TD
    A[ìŠ¤ì¼€ì¤„ëŸ¬<br/>ë§¤ 2ì‹œê°„ ì‹¤í–‰] --> B[1ë‹¨ê³„: Sales Org ë°ì´í„° ìˆ˜ì§‘]
    B --> C[Redis Queue 1<br/>Sales Org ì‘ì—…]
    C --> D[Worker 1<br/>T2A API â†’ CSV]
    D --> E[CSV ë¨¸ì§€<br/>Material.csv, GSCM.csv]
    E --> F[2ë‹¨ê³„: Adaptive Card ìƒì„±]
    F --> G[Redis Queue 2<br/>Userë³„ ì‘ì—…]
    G --> H[Worker 2<br/>ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰]
    H --> I[PostgreSQL ì €ì¥<br/>Adaptive Card ë°ì´í„°]
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

- **Redis Stream**: Consumer Group ê¸°ë°˜ ë©”ì‹œì§€ í
- **PostgreSQL**: Adaptive Card ë°ì´í„° ì €ì¥ì†Œ
- **ìŠ¤ì¼€ì¤„ëŸ¬**: APScheduler ê¸°ë°˜ ìë™ ì‹¤í–‰
- **ì›Œì»¤**: ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# PostgreSQL ì„¤ì¹˜ ë° ì‹œì‘ (macOS)
brew install postgresql@15
brew services start postgresql@15

# Redis ì„¤ì¹˜ ë° ì‹œì‘ (macOS)
brew install redis
brew services start redis

# ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
createdb adaptive_card_db
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
python test_overall.py

# ìˆ˜í‰ í™•ì¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¶Œì¥)
./start_scalable.sh

# ë˜ëŠ” ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python -m app.scheduler
```

### 3. Docker ì‹¤í–‰ (ì„ íƒì‚¬í•­)

```bash
# Docker Composeë¡œ ì‹¤í–‰
docker-compose -f docker-compose.full.yml up --build
```

## âš™ï¸ ì„¤ì •

### í™˜ê²½ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|--------|--------|------|
| `MAX_WORKERS_SALES` | `3` | Sales Org ì›Œì»¤ ìˆ˜ |
| `MAX_WORKERS_ADAPTIVE` | `3` | Adaptive Card ì›Œì»¤ ìˆ˜ |
| `CRON_SPEC` | `0 */2 * * *` | ì‹¤í–‰ ì£¼ê¸° (2ì‹œê°„ë§ˆë‹¤) |
| `REDIS_URL` | `redis://localhost:6379/0` | Redis ì—°ê²° URL |
| `DB_HOST` | `localhost` | PostgreSQL í˜¸ìŠ¤íŠ¸ |
| `DB_PORT` | `5432` | PostgreSQL í¬íŠ¸ |
| `DB_NAME` | `adaptive_card_db` | ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ |
| `T2A_API_URL` | `https://api.t2a.example.com` | T2A API URL |

### ì›Œì»¤ ìŠ¤ì¼€ì¼ë§

```bash
# 5ê°œ ì›Œì»¤ë¡œ ìŠ¤ì¼€ì¼ë§
MAX_WORKERS_SALES=5 MAX_WORKERS_ADAPTIVE=5 ./start_scalable.sh

# í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
export MAX_WORKERS_SALES=10
export MAX_WORKERS_ADAPTIVE=10
./start_scalable.sh
```

## ğŸ“– ì‚¬ìš©ë²•

### ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰

```bash
# ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ (ë‹¨ì¼ ì›Œì»¤)
python -m app.scheduler

# ìˆ˜í‰ í™•ì¥ ìŠ¤ì¼€ì¤„ëŸ¬ (ê¶Œì¥)
python -m app.scalable_scheduler
```

### ì›Œì»¤ ê°œë³„ ì‹¤í–‰

```bash
# Sales Org ì›Œì»¤
python -m app.worker_sales_org

# Adaptive Card ì›Œì»¤
python -m app.worker_adaptive_card
```

### í…ŒìŠ¤íŠ¸ ë„êµ¬

```bash
python test_overall.py

# ë©”ë‰´ ì˜µì…˜:
# 1. í ìƒíƒœ í™•ì¸
# 2. í ì´ˆê¸°í™”
# 3. íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
# 4. ëª¨ë‹ˆí„°ë§ (2ë¶„)
# 5. ì›Œì»¤ í…ŒìŠ¤íŠ¸
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### í ìƒíƒœ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
python test_overall.py
# ì˜µì…˜ 4 ì„ íƒ

# ë˜ëŠ” ì§ì ‘ í™•ì¸
python -c "
from app.stream_monitor import StreamMonitor
from app import config
import redis

r = redis.from_url(config.REDIS_URL, decode_responses=True)
monitor = StreamMonitor(r)
status = monitor.get_pipeline_status(
    config.SALES_ORG_QUEUE, config.SALES_ORG_GROUP,
    config.ADAPTIVE_CARD_QUEUE, config.ADAPTIVE_CARD_GROUP
)
print(f'Pipeline Status: {status[\"pipeline_status\"]}')
"
```

### Redis ëª…ë ¹ì–´

```bash
# í ê¸¸ì´ í™•ì¸
redis-cli XLEN sales_org_queue
redis-cli XLEN adaptive_card_queue

# Pending ë©”ì‹œì§€ í™•ì¸
redis-cli XPENDING sales_org_queue sales_org_group
redis-cli XPENDING adaptive_card_queue adaptive_card_group

# Consumer ì •ë³´ í™•ì¸
redis-cli XINFO CONSUMERS sales_org_queue sales_org_group
```

### ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f logs/scheduler.log

# íŠ¹ì • ì›Œì»¤ ë¡œê·¸ í•„í„°ë§
grep "worker-1" logs/worker.log
```

## ğŸ”§ ê°œë°œ ê°€ì´ë“œ

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Data_Engineering/
â”œâ”€â”€ app/                          # ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
â”‚   â”œâ”€â”€ scheduler.py              # ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ scalable_scheduler.py     # ìˆ˜í‰ í™•ì¥ ìŠ¤ì¼€ì¤„ëŸ¬ (ê¶Œì¥)
â”‚   â”œâ”€â”€ worker_sales_org.py       # Sales Org ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ worker_adaptive_card.py   # Adaptive Card ìƒì„±
â”‚   â”œâ”€â”€ retry_handler.py          # ì¬ì‹œë„ ë¡œì§ ë° Dead Letter Queue
â”‚   â”œâ”€â”€ stream_monitor.py         # Redis Stream ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ config.py                 # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ src/                          # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”œâ”€â”€ analysis_main.py          # ë¶„ì„ ì—”ì§„
â”‚   â”œâ”€â”€ scenario_preprocessor.py  # ì‹œë‚˜ë¦¬ì˜¤ ì „ì²˜ë¦¬
â”‚   â””â”€â”€ action_registry.py        # ì•¡ì…˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬
â”œâ”€â”€ test_overall.py               # í…ŒìŠ¤íŠ¸ ë„êµ¬
â”œâ”€â”€ start_scalable.sh             # ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md
```

### í•µì‹¬ ê¸°ëŠ¥

#### 1. Redis Stream ê´€ë¦¬
- **Consumer Group**: ë¡œë“œ ë°¸ëŸ°ì‹±
- **ACK ê¸°ë°˜**: ì•ˆì •ì ì¸ ë©”ì‹œì§€ ì²˜ë¦¬
- **Dead Letter Queue**: ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ê´€ë¦¬

#### 2. ì¬ì‹œë„ ë¡œì§
- **ì§€ìˆ˜ ë°±ì˜¤í”„**: ì ì§„ì  ì¬ì‹œë„ ê°„ê²©
- **ìµœëŒ€ ì¬ì‹œë„**: 3íšŒ ì‹œë„ í›„ DLQ ì´ë™
- **ìë™ ë³µêµ¬**: ì¼ì‹œì  ì˜¤ë¥˜ ìë™ ì²˜ë¦¬

#### 3. ìˆ˜í‰ í™•ì¥
- **ë³‘ë ¬ ì›Œì»¤**: ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë™ì‹œ ì‹¤í–‰
- **ë™ì  ìŠ¤ì¼€ì¼ë§**: í™˜ê²½ë³€ìˆ˜ë¡œ ì›Œì»¤ ìˆ˜ ì¡°ì •
- **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: ThreadPoolExecutor ê¸°ë°˜

### ìƒˆë¡œìš´ ì›Œì»¤ ì¶”ê°€

```python
# app/worker_template.py
import os, sys, logging, redis, socket
from pathlib import Path

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from app import config
from app.retry_handler import RetryHandler, retry_on_failure

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("worker_template")

# Redis ì—°ê²°
r = redis.from_url(config.REDIS_URL, decode_responses=True)
STREAM = "your_queue_name"
GROUP = "your_group_name"
WORKER_NAME = os.getenv("WORKER_NAME", f"template_worker-{socket.gethostname()}-{os.getpid()}")
CONSUMER = WORKER_NAME

# ì¬ì‹œë„ í•¸ë“¤ëŸ¬
retry_handler = RetryHandler(r, STREAM, GROUP, CONSUMER)

def ensure_group():
    try:
        r.xgroup_create(STREAM, GROUP, id="0", mkstream=True)
        log.info(f"[{WORKER_NAME}] XGROUP CREATE {STREAM} {GROUP}")
    except redis.exceptions.ResponseError as e:
        if "BUSYGROUP" not in str(e):
            raise

@retry_on_failure(max_retries=3, base_delay=1.0)
def _process_data(data):
    """ë°ì´í„° ì²˜ë¦¬ ë¡œì§"""
    log.info(f"[{WORKER_NAME}] Processing data={data}")
    # ì—¬ê¸°ì— ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
    log.info(f"[{WORKER_NAME}] Completed data={data}")

def handle_message(message_id, fields):
    """ë©”ì‹œì§€ ì²˜ë¦¬"""
    data = fields.get("data")
    
    success = retry_handler.process_with_retry(
        message_id=message_id,
        fields=fields,
        handler_func=lambda mid, f: _process_data(f.get("data")),
        max_retries=3,
        base_delay=1.0,
        max_delay=60.0
    )
    
    if not success:
        log.error(f"[{WORKER_NAME}] Failed to process message {message_id}")

def run_batch_once():
    ensure_group()
    log.info(f"[{WORKER_NAME}] batch start: STREAM={STREAM} GROUP={GROUP}")
    
    while True:
        resp = r.xreadgroup(
            GROUP, CONSUMER,
            streams={STREAM: ">"},
            count=config.BATCH_COUNT,
            block=1000
        )
        
        if resp:
            for _, messages in resp:
                for message_id, fields in messages:
                    handle_message(message_id, fields)
            continue
            
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        xlen = r.xlen(STREAM)
        pend_info = r.xpending(STREAM, GROUP) or {}
        pending = pend_info.get("count", 0)
        
        if xlen == 0 and pending == 0:
            log.info(f"[{WORKER_NAME}] queue drained â†’ exit")
            break
            
        time.sleep(0.5)

def main():
    run_batch_once()

if __name__ == "__main__":
    main()
```

### ì„¤ì • ì¶”ê°€

```python
# app/config.pyì— ì¶”ê°€
YOUR_QUEUE_NAME = os.getenv("YOUR_QUEUE_NAME", "your_queue")
YOUR_GROUP_NAME = os.getenv("YOUR_GROUP_NAME", "your_group")
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### 1. Redis ì—°ê²° ì‹¤íŒ¨
```bash
# Redis ìƒíƒœ í™•ì¸
redis-cli ping

# ì—°ê²° ì •ë³´ í™•ì¸
echo $REDIS_URL
```

#### 2. PostgreSQL ì—°ê²° ì‹¤íŒ¨
```bash
# PostgreSQL ìƒíƒœ í™•ì¸
psql -h localhost -U postgres -d adaptive_card_db -c "SELECT 1;"

# ì—°ê²° ì •ë³´ í™•ì¸
echo $DB_HOST $DB_PORT $DB_NAME
```

#### 3. ì›Œì»¤ê°€ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
```bash
# í ìƒíƒœ í™•ì¸
redis-cli XLEN sales_org_queue
redis-cli XPENDING sales_org_queue sales_org_group

# Consumer ìƒíƒœ í™•ì¸
redis-cli XINFO CONSUMERS sales_org_queue sales_org_group
```

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ì›Œì»¤ ìˆ˜ ì¤„ì´ê¸°
MAX_WORKERS_SALES=1 MAX_WORKERS_ADAPTIVE=1 ./start_scalable.sh
```

### ë¡œê·¸ ë¶„ì„

```bash
# ì—ëŸ¬ ë¡œê·¸ í™•ì¸
grep "ERROR" logs/*.log

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸
grep "2024-01-01 10:" logs/scheduler.log

# ì›Œì»¤ë³„ ì„±ëŠ¥ ë¶„ì„
grep "Completed" logs/worker.log | wc -l
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ì›Œì»¤ ìˆ˜ íŠœë‹

```bash
# CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ê¶Œì¥ ì„¤ì •
CPU_CORES=$(nproc)
MAX_WORKERS_SALES=$((CPU_CORES / 2))
MAX_WORKERS_ADAPTIVE=$((CPU_CORES / 2))

# ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¤ì • (8GB ê¸°ì¤€)
MAX_WORKERS_SALES=4 MAX_WORKERS_ADAPTIVE=4 ./start_scalable.sh
```

### Redis ìµœì í™”

```bash
# Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
redis-cli INFO memory

# Stream ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
redis-cli MEMORY USAGE sales_org_queue
redis-cli MEMORY USAGE adaptive_card_queue
```

### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

```sql
-- ì¸ë±ìŠ¤ ì¶”ê°€
CREATE INDEX IF NOT EXISTS idx_user_id ON user_scenario_result(user_id);
CREATE INDEX IF NOT EXISTS idx_created_at ON user_scenario_result(created_at);

-- í…Œì´ë¸” í¬ê¸° í™•ì¸
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:

- ğŸ“§ ì´ë©”ì¼: support@example.com
- ğŸ› ì´ìŠˆ: [GitHub Issues](https://github.com/your-repo/issues)
- ğŸ“– ë¬¸ì„œ: [Wiki](https://github.com/your-repo/wiki)

---

**ğŸ‰ Happy Coding!**