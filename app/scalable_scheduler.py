# app/scalable_scheduler.py

import os, sys, time, logging, redis
import subprocess
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ê²½ë¡œ / ì„¤ì •
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)
from app import config
from app.window import current_window_id
from src.modules.agents.analysis.orm.oracle import ORACLE

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("scalable_scheduler")
for name in ["apscheduler", "apscheduler.scheduler", "apscheduler.executors.default", "apscheduler.jobstores.default"]:
    logging.getLogger(name).setLevel(logging.WARNING)

# Redis ì—°ê²°
r = redis.from_url(config.REDIS_URL, decode_responses=True)

class ScalableScheduler:
    """
    ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¤„ëŸ¬
    - ì—¬ëŸ¬ ì›Œì»¤ ì¸ìŠ¤í„´ìŠ¤ ë™ì‹œ ì‹¤í–‰
    - Consumer Groupì„ í†µí•œ ë¡œë“œ ë°¸ëŸ°ì‹±
    - ë™ì  ì›Œì»¤ ìˆ˜ ì¡°ì •
    """
    
    def __init__(self):
        self.max_workers_sales = int(os.getenv("MAX_WORKERS_SALES", "3"))
        self.max_workers_adaptive = int(os.getenv("MAX_WORKERS_ADAPTIVE", "3"))
        self.executor = ThreadPoolExecutor(max_workers=max(self.max_workers_sales, self.max_workers_adaptive))
        
    def enqueue_sales_org_jobs(self, sales_org_ids):
        """Sales Org ì‘ì—…ë“¤ì„ íì— ë“±ë¡"""
        window_id, _, _ = current_window_id()
        for sid in sales_org_ids:
            r.xadd(
                config.SALES_ORG_QUEUE,
                {
                    "sales_org_id": sid,
                    "window_id": window_id,
                    "stage": "sales_org_fetch",
                    "created_at": time.time(),
                    "retry_count": "0"  # ì¬ì‹œë„ ì¹´ìš´í„° ì´ˆê¸°í™”
                },
                maxlen=5000, approximate=True
            )
        log.info(f"[enqueue] sales_org jobs={len(sales_org_ids)}  xlen={r.xlen(config.SALES_ORG_QUEUE)}")
        
    def enqueue_adaptive_card_jobs(self, user_ids):
        """Adaptive Card ì‘ì—…ë“¤ì„ íì— ë“±ë¡"""
        window_id, _, _ = current_window_id()
        for uid in user_ids:
            r.xadd(
                config.ADAPTIVE_CARD_QUEUE,
                {
                    "user_id": uid[0],
                    "window_id": window_id,
                    "stage": "adaptive_card_generation",
                    "created_at": time.time(),
                    "retry_count": "0"  # ì¬ì‹œë„ ì¹´ìš´í„° ì´ˆê¸°í™”
                },
                maxlen=5000, approximate=True
            )
        log.info(f"[enqueue] adaptive_card jobs={len(user_ids)}  xlen={r.xlen(config.ADAPTIVE_CARD_QUEUE)}")

    def run_worker_batch(self, worker_module, worker_count, timeout_seconds=1800):
        """
        ì—¬ëŸ¬ ì›Œì»¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
        
        Args:
            worker_module: ì‹¤í–‰í•  ì›Œì»¤ ëª¨ë“ˆëª…
            worker_count: ì‹¤í–‰í•  ì›Œì»¤ ìˆ˜
            timeout_seconds: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        log.info(f"[spawn] Starting {worker_count} instances of {worker_module}")
        
        # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ë“¤ì„ ë³‘ë ¬ë¡œ ì‹œì‘
        futures = []
        for i in range(worker_count):
            future = self.executor.submit(
                self._run_single_worker, 
                worker_module, 
                f"worker-{i+1}"
            )
            futures.append(future)
        
        # ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì ìš©)
        completed_count = 0
        for future in as_completed(futures, timeout=timeout_seconds):
            try:
                result = future.result()
                completed_count += 1
                log.info(f"[spawn] Worker completed: {result}")
            except Exception as e:
                log.error(f"[spawn] Worker failed: {e}")
        
        log.info(f"[spawn] {completed_count}/{worker_count} workers completed for {worker_module}")

    def _run_single_worker(self, worker_module, worker_name):
        """ë‹¨ì¼ ì›Œì»¤ ì‹¤í–‰"""
        try:
            # í™˜ê²½ë³€ìˆ˜ë¡œ ì›Œì»¤ ì´ë¦„ ì„¤ì •
            env = dict(os.environ, PYTHONUNBUFFERED="1", WORKER_NAME=worker_name)
            
            result = subprocess.run(
                [sys.executable, "-m", worker_module],
                cwd=PROJECT_ROOT,
                check=True,
                capture_output=True,
                text=True,
                timeout=1800,  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
                env=env
            )
            
            return f"{worker_name}: SUCCESS"
            
        except subprocess.TimeoutExpired:
            log.error(f"[spawn] {worker_name} timed out after 30 minutes")
            return f"{worker_name}: TIMEOUT"
        except subprocess.CalledProcessError as e:
            log.error(f"[spawn] {worker_name} failed with return code {e.returncode}")
            return f"{worker_name}: FAILED ({e.returncode})"
        except Exception as e:
            log.error(f"[spawn] {worker_name} exception: {e}")
            return f"{worker_name}: EXCEPTION"

    def wait_for_queue_completion(self, queue_name, group_name, max_wait_seconds=1800):
        """í ì™„ë£Œ ëŒ€ê¸° (ê°„ì†Œí™”)"""
        log.info(f"[wait] Waiting for {queue_name} completion...")
        start_time = time.time()
        
        while time.time() - start_time < max_wait_seconds:
            stream_length = r.xlen(queue_name)
            pending_info = r.xpending(queue_name, group_name)
            pending_count = pending_info.get("count", 0) if pending_info else 0
            
            if stream_length == 0 and pending_count == 0:
                log.info(f"[wait] {queue_name} completed!")
                return True
                
            time.sleep(5)  # 5ì´ˆë§ˆë‹¤ í™•ì¸
        
        log.warning(f"[wait] {queue_name} timeout after {max_wait_seconds}s")
        return False

    def do_merge_after_sales_org(self):
        """Sales Org ì‘ì—… ì™„ë£Œ í›„ CSV ë¨¸ì§€"""
        from app.scheduler import do_merge_after_sales_org as merge_func
        merge_func()

    def run_scalable_pipeline(self):
        """ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        log.info("ğŸš€ Scalable pipeline start")
        
        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘
            ora = ORACLE()
            sales_org_ids = ora.fetch_sales_org_code()
            user_ids = ora.fetch_ai_users()
            
            # 2. Sales Org ë‹¨ê³„
            log.info(f"[pipeline] Phase 1: Processing {len(sales_org_ids)} sales orgs with {self.max_workers_sales} workers")
            self.enqueue_sales_org_jobs(sales_org_ids)
            self.run_worker_batch("app.worker_sales_org", self.max_workers_sales)
            
            # í ì™„ë£Œ ëŒ€ê¸°
            self.wait_for_queue_completion(config.SALES_ORG_QUEUE, config.SALES_ORG_GROUP)
            
            # 3. ë¨¸ì§€
            self.do_merge_after_sales_org()
            
            # 4. Adaptive Card ë‹¨ê³„
            log.info(f"[pipeline] Phase 2: Processing {len(user_ids)} users with {self.max_workers_adaptive} workers")
            self.enqueue_adaptive_card_jobs(user_ids)
            self.run_worker_batch("app.worker_adaptive_card", self.max_workers_adaptive)
            
            # í ì™„ë£Œ ëŒ€ê¸°
            self.wait_for_queue_completion(config.ADAPTIVE_CARD_QUEUE, config.ADAPTIVE_CARD_GROUP)
            
            log.info("âœ… Scalable pipeline finished")
            
        except Exception as e:
            log.error(f"âŒ Pipeline failed: {e}")
            raise

    def cleanup_queues(self):
        """í ì •ë¦¬"""
        log.info("ğŸ§¹ Cleaning up queues...")
        try:
            r.delete(config.SALES_ORG_QUEUE)
            r.delete(config.ADAPTIVE_CARD_QUEUE)
            log.info("âœ… Queue cleanup completed")
        except Exception as e:
            log.error(f"âŒ Queue cleanup failed: {e}")

    def get_queue_stats(self):
        """í ìƒíƒœ ì¡°íšŒ (ê°„ì†Œí™”)"""
        try:
            return {
                "sales_org_queue": {
                    "length": r.xlen(config.SALES_ORG_QUEUE),
                    "pending": r.xpending(config.SALES_ORG_QUEUE, config.SALES_ORG_GROUP).get("count", 0)
                },
                "adaptive_card_queue": {
                    "length": r.xlen(config.ADAPTIVE_CARD_QUEUE),
                    "pending": r.xpending(config.ADAPTIVE_CARD_QUEUE, config.ADAPTIVE_CARD_GROUP).get("count", 0)
                }
            }
        except:
            return {"sales_org_queue": {"length": 0, "pending": 0}, "adaptive_card_queue": {"length": 0, "pending": 0}}

    def shutdown(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ"""
        log.info("ğŸ›‘ Shutting down scalable scheduler...")
        self.executor.shutdown(wait=True)
        log.info("âœ… Scheduler shutdown completed")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log.info(f"[bootstrap] workspace={PROJECT_ROOT}")
    
    scheduler = ScalableScheduler()
    
    # ì‹œì‘ ì „ í ìƒíƒœ ë¡œê¹…
    stats = scheduler.get_queue_stats()
    log.info(f"[bootstrap] Initial queue stats: {stats}")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    sched = BlockingScheduler(timezone=config.TZ)
    m, h, d, mo, dow = config.CRON_SPEC.split()
    cron = CronTrigger(minute=m, hour=h, day=d, month=mo, day_of_week=dow)
    
    sched.add_job(
        scheduler.run_scalable_pipeline,
        trigger=cron,
        id="scalable-pipeline",
        coalesce=True,
        misfire_grace_time=300,
        replace_existing=True,
        max_instances=1,
    )
    
    log.info(f"[bootstrap] Scheduled with CRON_SPEC='{config.CRON_SPEC}'")
    log.info(f"[bootstrap] Max workers - Sales: {scheduler.max_workers_sales}, Adaptive: {scheduler.max_workers_adaptive}")
    
    # í ì •ë¦¬
    scheduler.cleanup_queues()
    
    try:
        sched.start()
    except KeyboardInterrupt:
        log.info("Received interrupt signal")
    finally:
        scheduler.shutdown()

if __name__ == "__main__":
    main()
