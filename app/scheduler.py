# app/scheduler.py
 
import os, sys, time, logging, re
import pandas as pd
import redis
import subprocess
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
from pathlib import Path
 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œì íŠ¸ ê²½ë¡œ / ì„¤ì •
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)
from app import config
from app.window import current_window_id
from src.modules.agents.analysis.orm.oracle import ORACLE
 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… (ìš°ë¦¬ë§Œ INFO, APSchedulerëŠ” WARNINGìœ¼ë¡œ ì •ë¦¬)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("scheduler")
for name in ["apscheduler", "apscheduler.scheduler", "apscheduler.executors.default", "apscheduler.jobstores.default"]:
   logging.getLogger(name).setLevel(logging.WARNING)
   
# Redis ì—°ê²°
r = redis.from_url(config.REDIS_URL, decode_responses=True)
 
# enqueue ë‹¨ê³„
def enqueue_sales_org_jobs(sales_org_ids):
   window_id, _, _ = current_window_id()
   for sid in sales_org_ids:
       r.xadd(
           config.SALES_ORG_QUEUE,
           {
               "sales_org_id": sid,
               "window_id": window_id,
               "stage": "sales_org_fetch",
               "created_at": time.time(),
           },
           maxlen=5000, approximate=True # í­ì£¼ ë°©ì§€
       )
   log.info(f"[enqueue] sales_org jobs={len(sales_org_ids)}  xlen={r.xlen(config.SALES_ORG_QUEUE)}")
   
def enqueue_adaptive_card_jobs(user_ids):
   window_id, _, _ = current_window_id()
   for uid in user_ids:
       r.xadd(
           config.ADAPTIVE_CARD_QUEUE,
           {
               "user_id": uid[0],
               "window_id": window_id,
               "stage": "adaptive_card_generation",
               "created_at": time.time(),
           },
           maxlen=5000, approximate=True
       )
   log.info(f"[enqueue] adaptive_card jobs={len(user_ids)}  xlen={r.xlen(config.ADAPTIVE_CARD_QUEUE)}")
 
def clear_queues(r):
    """ê¸°ì¡´ í ë°ì´í„° ì •ë¦¬"""
    try:
        r.delete(config.SALES_ORG_QUEUE)
        r.delete(config.ADAPTIVE_CARD_QUEUE)
        log.info("ğŸ§¹ í ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        log.error(f"âŒ í ì •ë¦¬ ì‹¤íŒ¨: {e}")
 
def do_merge_after_sales_org():
   """Material/GSCM CSV íŒŒì¼ë“¤ì„ ë¨¸ì§€"""
   window_id, _, _ = current_window_id()
   out_dir = Path(config.CSV_OUTPUT_DIR)
   log.info(f"[merge] Starting merge for window_id='{window_id}'")
   
   for prefix in ("Material", "GSCM"):
       files = sorted(out_dir.glob(f"{prefix}_*.csv"))
       if not files:
           continue
           
       dfs = []
       for f in files:
           try:
               dfs.append(pd.read_csv(f))
           except Exception as e:
               log.warning(f"[merge] Skip {f.name}: {e}")
               
       if not dfs:
           continue
           
       merged = pd.concat(dfs, ignore_index=True)
       tmp = out_dir / f"{prefix}_MERGED_{window_id}.csv.tmp"
       final = out_dir / f"{prefix}.csv"
       merged.to_csv(tmp, index=False)
       tmp.replace(final)
       log.info(f"[merge] {prefix} merged: {len(merged)} rows from {len(dfs)} files")
 
def _run_worker_once(py_module: str):
   log.info(f"[spawn] python -m {py_module}")
   subprocess.run(
       [sys.executable, "-m", py_module],
       cwd=PROJECT_ROOT,
       check=True,              # ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸
       env=dict(os.environ, PYTHONUNBUFFERED="1"),  # ë²„í¼ë§ ë°©ì§€(ê¶Œì¥)
   )
   log.info(f"[spawn] {py_module} finished")
   
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íŒŒì´í”„ë¼ì¸ 1íšŒ ì‹¤í–‰(ì›ìƒ·): 1ì°¨ â†’ ëŒ€ê¸° â†’ ë¨¸ì§€ â†’ 2ì°¨ â†’ ëŒ€ê¸°
 
def run_pipeline_once():
   log.info("ğŸš€ pipeline start")
   ora = ORACLE()
   sales_org_ids = ora.fetch_sales_org_code()
   user_ids      = ora.fetch_ai_users()
   
   # 1) í1 ì ì¬
   enqueue_sales_org_jobs(sales_org_ids)
   # 2) ì›Œì»¤1 í•œ ë²ˆ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ í›„ ìë™ ì¢…ë£Œ)
   _run_worker_once("app.worker_sales_org")
   # 3) ë¨¸ì§€
   do_merge_after_sales_org()
   # 4) í2 ì ì¬
   enqueue_adaptive_card_jobs(user_ids)
   # 5) ì›Œì»¤2 í•œ ë²ˆ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ í›„ ìë™ ì¢…ë£Œ)
   _run_worker_once("app.worker_adaptive_card")
   
   log.info("âœ… pipeline finished")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìŠ¤ì¼€ì¤„ëŸ¬ ì—”íŠ¸ë¦¬
def main():
   log.info(f"[bootstrap] workspace={PROJECT_ROOT}")
   log.info(f"[bootstrap] queues before start: sales={r.xlen(config.SALES_ORG_QUEUE)}, adaptive={r.xlen(config.ADAPTIVE_CARD_QUEUE)}")
   sched = BlockingScheduler(timezone=config.TZ)
   # CRON_SPEC ì˜ˆ) "* * * * *" (ë§¤ë¶„), "0/30 * * * *" (30ì´ˆë§ˆë‹¤), "0 */2 * * *" (2ì‹œê°„ ê°„ê²©)
   m, h, d, mo, dow = config.CRON_SPEC.split()
   cron = CronTrigger(minute=m, hour=h, day=d, month=mo, day_of_week=dow)
   sched.add_job(
       run_pipeline_once,
       trigger=cron,
       id="pipeline-once",
       coalesce=True,
       misfire_grace_time=300,
       replace_existing=True,
       max_instances=1,  # ì´ì „ ì‹¤í–‰ì´ ëë‚˜ì§€ ì•Šì•˜ìœ¼ë©´ ê²¹ì¹˜ì§€ ì•Šë„ë¡
   )
   log.info(f"[bootstrap] scheduled with CRON_SPEC='{config.CRON_SPEC}'")
   
   clear_queues(r)
   sched.start()
 
if __name__ == "__main__":
   main()